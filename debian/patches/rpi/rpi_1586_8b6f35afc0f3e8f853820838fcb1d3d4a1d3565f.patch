commit 8b6f35afc0f3e8f853820838fcb1d3d4a1d3565f
Author: Don Zickus <dzickus@redhat.com>
Date:   Tue Jan 24 15:17:53 2017 -0800

    kernel/watchdog: prevent false hardlockup on overloaded system
    
    
    [ Upstream commit b94f51183b0617e7b9b4fb4137d4cf1cab7547c2 ]
    
    On an overloaded system, it is possible that a change in the watchdog
    threshold can be delayed long enough to trigger a false positive.
    
    This can easily be achieved by having a cpu spinning indefinitely on a
    task, while another cpu updates watchdog threshold.
    
    What happens is while trying to park the watchdog threads, the hrtimers
    on the other cpus trigger and reprogram themselves with the new slower
    watchdog threshold.  Meanwhile, the nmi watchdog is still programmed
    with the old faster threshold.
    
    Because the one cpu is blocked, it prevents the thread parking on the
    other cpus from completing, which is needed to shutdown the nmi watchdog
    and reprogram it correctly.  As a result, a false positive from the nmi
    watchdog is reported.
    
    Fix this by setting a park_in_progress flag to block all lockups until
    the parking is complete.
    
    Fix provided by Ulrich Obergfell.
    
    [akpm@linux-foundation.org: s/park_in_progress/watchdog_park_in_progress/]
    Link: http://lkml.kernel.org/r/1481041033-192236-1-git-send-email-dzickus@redhat.com
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    Reviewed-by: Aaron Tomlin <atomlin@redhat.com>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    Signed-off-by: Sasha Levin <alexander.levin@verizon.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/linux/nmi.h b/include/linux/nmi.h
index aacca82..0a3fadc 100644
--- a/include/linux/nmi.h
+++ b/include/linux/nmi.h
@@ -110,6 +110,7 @@ extern int watchdog_user_enabled;
 extern int watchdog_thresh;
 extern unsigned long watchdog_enabled;
 extern unsigned long *watchdog_cpumask_bits;
+extern atomic_t watchdog_park_in_progress;
 #ifdef CONFIG_SMP
 extern int sysctl_softlockup_all_cpu_backtrace;
 extern int sysctl_hardlockup_all_cpu_backtrace;
diff --git a/kernel/watchdog.c b/kernel/watchdog.c
index d4b0fa0..63177be 100644
--- a/kernel/watchdog.c
+++ b/kernel/watchdog.c
@@ -49,6 +49,8 @@ unsigned long *watchdog_cpumask_bits = cpumask_bits(&watchdog_cpumask);
 #define for_each_watchdog_cpu(cpu) \
 	for_each_cpu_and((cpu), cpu_online_mask, &watchdog_cpumask)
 
+atomic_t watchdog_park_in_progress = ATOMIC_INIT(0);
+
 /*
  * The 'watchdog_running' variable is set to 1 when the watchdog threads
  * are registered/started and is set to 0 when the watchdog threads are
@@ -260,6 +262,9 @@ static enum hrtimer_restart watchdog_timer_fn(struct hrtimer *hrtimer)
 	int duration;
 	int softlockup_all_cpu_backtrace = sysctl_softlockup_all_cpu_backtrace;
 
+	if (atomic_read(&watchdog_park_in_progress) != 0)
+		return HRTIMER_NORESTART;
+
 	/* kick the hardlockup detector */
 	watchdog_interrupt_count();
 
@@ -467,12 +472,16 @@ static int watchdog_park_threads(void)
 {
 	int cpu, ret = 0;
 
+	atomic_set(&watchdog_park_in_progress, 1);
+
 	for_each_watchdog_cpu(cpu) {
 		ret = kthread_park(per_cpu(softlockup_watchdog, cpu));
 		if (ret)
 			break;
 	}
 
+	atomic_set(&watchdog_park_in_progress, 0);
+
 	return ret;
 }
 
diff --git a/kernel/watchdog_hld.c b/kernel/watchdog_hld.c
index 84016c8..12b8dd6 100644
--- a/kernel/watchdog_hld.c
+++ b/kernel/watchdog_hld.c
@@ -84,6 +84,9 @@ static void watchdog_overflow_callback(struct perf_event *event,
 	/* Ensure the watchdog never gets throttled */
 	event->hw.interrupts = 0;
 
+	if (atomic_read(&watchdog_park_in_progress) != 0)
+		return;
+
 	if (__this_cpu_read(watchdog_nmi_touch) == true) {
 		__this_cpu_write(watchdog_nmi_touch, false);
 		return;
